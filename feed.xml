<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ak4774blog.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ak4774blog.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-02T20:19:13+00:00</updated><id>https://ak4774blog.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">basic matrix operations (artin 1.1)</title><link href="https://ak4774blog.github.io/blog/2025/matrices-the-basic-operations/" rel="alternate" type="text/html" title="basic matrix operations (artin 1.1)"/><published>2025-07-01T00:00:00+00:00</published><updated>2025-07-01T00:00:00+00:00</updated><id>https://ak4774blog.github.io/blog/2025/matrices-the-basic-operations</id><content type="html" xml:base="https://ak4774blog.github.io/blog/2025/matrices-the-basic-operations/"><![CDATA[<p>This blog post, as well as the upcoming posts, will generally follow the format of the book (Artin Algebra e2) itself. This is an exposition on Chapter 1, 1.1 on the basic operations of Matrices.</p> <h2 id="notes-on-chapter">Notes on Chapter</h2> <p>A $m \times n$ <em>matrix</em> for $m, n \in \mathbb{Z}^+$ is simply a rectangular array of $mn$ integers:</p> \[\begin{array}{cc} &amp; n \text{ columns} \\ m \text{ rows} &amp; \begin{bmatrix} a_{11} &amp; \ldots &amp; a_{1n} \\ \vdots &amp; &amp; \vdots \\ a_{m1} &amp; \ldots &amp; a_{mn} \end{bmatrix} \end{array}\] <hr/> <p>Some definitions about matrices:</p> <ul> <li>The numbers $a_{ij}$ are called the <em>matrix entries</em> <ul> <li>The index $i$ is the <em>row index</em> and j is defined similarly as the <em>column index</em> where $i \in [1,m]$ and $j \in [1,n].$</li> </ul> </li> <li>A $n\times n$ matrix is called a <em>square matrix</em>.</li> <li>A $1\times n$ matrix is called a <em>row vector</em>, represented as</li> </ul> \[[a_1 \dots a_n],\] <p>and a $n\times 1$ matrix is called a <em>column vector</em>, represented as</p> \[\begin{bmatrix} a_1 \\ \vdots \\ a_n \end{bmatrix}.\] <hr/> <p>We can <em>add matrices</em>. For two matrices with the same dimensions (the only case in which we can add matrices), say $A = (a_{ij}), B= (b_{ij})$, we can just add each element in the vectors. Formally, for each element $s_{ij} \in X$, where $X = A+B$, $s_{ij} = a_{ij} + b_{ij}$.</p> <p>We can also do <em>scalar multiplication</em>, where we multiply a matrix $A$ by a scalar $b$. This is also similarly easy to do: for the new matrix $C$, $C_{ij} = A_{ij}b.$</p> <p><em>Matrix multiplication</em> is less trivial than the above two operations. There are a couple of cases for matrix multiplication.</p> <p>Case 1: Multiplying a column and row vector with the same dimension. This is fairly straightforward, just take $\sum_{n=1}^m a_n b_n.$ For example (courtesy of Artin),</p> \[\begin{bmatrix} 1 &amp; 3 &amp; 5 \end{bmatrix} \begin{bmatrix} 1 \\ -1 \\ 4 \end{bmatrix} = 1 - 3 + 20 = 18.\] <p>This type of matrix multiplication is useful for dimensional analysis used in science classes. For example, $(\text{grams}/\text{bar}) \cdot (\text{cost} / \text{gram}) = (\text{cost} / \text{gram}).$</p> <p>Case 2: Multiplying matrices with one having $n$ rows, and another having $n$ columns. First, note that if matrix $A$ has dimensions $l \times m$ and matrix $B$ has dimensions $m \times n$, then the product $A \times B$ will have dimensions $l \times n$. We can denote the product $A\times B$ as $P=(p_{ij})$. Then, $p_{ij}:= a_{i1}b_{1j} + a_{i2}b_{2j} + \cdots + a_{im}b_{mj}.$</p> <p>One useful application of matrix multiplication is to write linear equations nicely. For example, the system of equations:</p> \[\begin{array}{ccccccc} a_{11}x_1 &amp; + &amp; \cdots &amp; + &amp; a_{1n}x_n &amp; = &amp; b_1 \\ a_{21}x_1 &amp; + &amp; \cdots &amp; + &amp; a_{2n}x_n &amp; = &amp; b_2 \\ \vdots &amp; &amp; &amp; &amp; \vdots &amp; &amp; \vdots \\ a_{m1}x_1 &amp; + &amp; \cdots &amp; + &amp; a_{mn}x_n &amp; = &amp; b_m \end{array}\] <p>can be written as $AX = B$, where $A$ is the matrix of the coefficients of the equations, $X$ are the solutions and $B$ are the constants on the right side of the equations. For example (from Artin),</p> \[\begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 3 &amp; 5 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} 1 \\ 18 \end{bmatrix}\] <p>is:</p> \[\begin{align*} 2x_1 + x_2 &amp;= 1 \\ x_1 + 3x_2 + 5x_3 &amp;= 18. \end{align*}\] <p>Note that matrix multiplication is only defined when the number of columns of matrix $A$ is equal to the number of rows of matrix $B$.</p> <hr/> <p>Some identities of matrices include (for matrices $A$ and $B$):</p> <ul> <li>The <em>distributive law</em>: $A(B + B‚Äô) = AB + AB‚Äô,$ and $(A+A‚Äô)B = AB+A‚ÄôB$.</li> <li>The <em>associative law</em>: $(AB)C = A(BC)$. Sizes should be $A = l \times m$, $B=m\times n$, and $C = n\times p$ for some $l,m,n,p$. This implies that scalar multiplication can be represented as $c(AB) = (cA)B = A(cB)$.</li> <li>The <em>commutative law</em> does not hold for matrix multiplication, so $AB \ne BA$ (usually). <ul> <li>Even when both matrices are square, commutativity does not hold.</li> <li>When it occurs that $AB=BC$, $A$ and $C$ are said to <em>commute</em></li> </ul> </li> </ul> <p>Some additional definitions</p> <ul> <li>Matrices with all elements $0$ are a <em>zero matrix</em>.</li> <li>The diagonal $a_{ii}$ are its <em>diagonal entries</em>. <ul> <li>A matrix with its only nonzero entries being on the diagonal is called a <em>diagonal matrix</em>.</li> </ul> </li> <li>A $n\times n$ matrix with its diagonal elements being $1$ are called a <em>identity matrix</em> denoted $I_n,$ and satisfy $AI_n=A$ and $I_mA = A$. These are usually just denoted as $I$ in standard texts.</li> <li>A matrix in which the upper triangle is undetermined, while all other elements are $0$ is called a <em>upper triangular</em> matrix. Undetermined values in a matrix are denoted with * (an asterisk).</li> <li>Let $A$ be a square matrix with dimensions $n \times n$. If there exists a matrix $B$ such that</li> </ul> \[AB = I_n, BC = I_n,\] <p>then $B$ is the <em>inverse</em> of $A$, denoted $A^{-1}.$ A matrix $A$ that has a inverse is called a <em>invertible matrix</em>. Note that $AA^{-1} = I = A^{-1}A.$</p> <p><strong>Lemma (left and right inverse).</strong> Let $A$ be a square matrix that has a <em>right inverse</em>, a matrix $R$ such that $AR=I$, and also a <em>left inverse</em>, a matrix $L$ such that $LA = I$. Then $R=L$. So $A$ is invertible and $R$ is its inverse.</p> <p><em>Proof.</em> By definition, $R = IR$. Then, $R = LA(R) = L(AR)= LI= L$, as desired. $\square$</p> <p><strong>Proposition.</strong> Let $A$ and $B$ be invertible matrices. The product $AB$ and the inverse $A^{-1}$ are invertible, $(AB)^{-1}=B^{-1}A^{-1}$ and $(A^{-1})^{-1}=A$. If $A_1, \dots, A_m$ are invertible $n\times n$ matrices, the product $A_1\dots A_m$ is invertible, and its inverse is $A^{-1}_m\dots A^{-1}_1$.</p> <p>For a $2 \times 2$ matrix, its inverse is useful to memorize:</p> \[\begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d &amp; -b \\ -c &amp; a \end{bmatrix}.\] <p>Note that the denominator $ad-bc$ is the <em>determinant</em> of the matrix; if the determinant is $0$, the matrix is not invertible.</p> <p>The set of all invertible $n\times n$ matrices is called the $n$-dimensional <em>general linear group</em>.</p> <p><strong>Lemma.</strong> A square matrix that has either a row of zeros of a column of zeros is not invertible.</p> <p><em>Proof.</em> It suffices to check the two cases separately:</p> <p>Case 1: row of zeros. We claim that the matrix does not have a right inverse. If a row of $A$, a $n\times n$ matrix, is zero, and a similarly defined $B$, then the row of the product $AB$ is zero as well, as desired.</p> <p>Case 2: column of zeros. We claim that the matrix does not have a left inverse. A similar argument to the above case works here. $\square$</p> <hr/> <p>We can simplify matrix multiplication using various tricks. One of these tricks is called <em>block multiplication</em>. It allows us to simplify the multiplication by partitioning the matrices into smaller sub-matrices called ‚Äúblocks‚Äù. Then, you can treak these blocks as numbers, and do the multiplication easily.</p> <p>There are two key cases for this technique:</p> <p>Case 1: The row + column structure. We first take a matrix $M$ and split it vertically into two blocks $A$ and $B$ as such: $M = [A \mid B].$ We can then take another matrix $M‚Äô$ and split it horizontally into two blocks $A‚Äô$ and $B‚Äô$ (the author is too lazy to draw this out). Then, to multiply, you can use $MM‚Äô = AA‚Äô+BB‚Äô$, just like multiplying a column and row vector.</p> <p>Case 2: The 2x2 block structure. We can decompose into two different blocks, and multiply the same way as normal $2 \times 2$ matrices.</p> <hr/> <p><em>Matrix units</em> are the simplest nonzero matrices. The matrix unit $e_{ij}$ only has one nonzero element: a one in the position $i, j$. Matrices are usually denoted using uppercase letters, but a special exception for matrix units are given.</p> \[e_{ij} = \begin{array}{cc} &amp; j \\ i &amp; \left[ \begin{array}{ccc} &amp; \vdots &amp; \\ \cdots &amp; 1 &amp; \cdots \\ &amp; \vdots &amp; \end{array} \right] \end{array}\] <p>The set of matrix units is called a <em>basis</em> for the space of $m \times n$ matrices, because every $m\times n$ matrix $A = (a_{ij})$ is a <em>linear combination</em> of the matrices $e_{ij}$:</p> \[A = a_{11}e_{11} + a_{12}e_{12} + \dots = \sum_{i,j} a_{ij}e_{ij}.\] <p>To multiply a $m \times n$ matrix unit $e_{ij}$ and a $n\times p$ matrix unit $e_{jl}$, we can use the following formulas:</p> \[e_{ij}e_{jl} = e_{il}, e_{ij}e_{kl}=0 \quad \text{if} \quad j \ne k.\] <p>The column vector $e_i$, which has a single $1$ in its only nonzero position $i$ is a matrix unit, and the set ${e_1, \dots, e_n}$ is the <em>standard basis</em> of the $n$-dimensional vector space $\mathbb{R}^n.$ Multiplying these by column vectors is also well known. For a column vector $X = (x_1,\dots,x_n)$,</p> \[X = x_1e_1+\dots + x_ne_n = \sum_i x_ie_i.\] <p>To multiply matrix units and standard basis vectors, you can use the formulas:</p> \[e_{ij}e_{j} = e_{i}, e_{ij}e_{k} = 0 \quad \text{if} \quad j \ne k.\] <hr/> <h2 id="exercises">Exercises</h2> <p><strong>1.1.</strong> What are the entries $a_{21}$ and $a_{23}$ of the matrix:</p> \[A = \begin{bmatrix} 1 &amp; 2 &amp; 5 \\ 2 &amp; 7 &amp; 8 \\ 0 &amp; 9 &amp; 4 \end{bmatrix}?\] <p><em>Solution.</em> The entry $a_{21}$ is just the value in 2nd row and 1st column, or $\boxed{2}$. Similarly, the value in the 2nd row and 3rd column is $\boxed{8}.$ $\square$</p> <hr/> <p><strong>1.2.</strong> Determine the products $AB$ and $BA$ for the following values of $A$ and $B$: (a)</p> \[A = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 3 &amp; 1 &amp; 1 \end{bmatrix}, B = \begin{bmatrix} -8 &amp; -4 \\ 9 &amp; 5 \\ -3 &amp; -2 \end{bmatrix}\] <p>(b)</p> \[A = \begin{bmatrix} 1 &amp; 4 \end{bmatrix}, B = \begin{bmatrix} 6 &amp; -4 \\ 3 &amp; 2 \end{bmatrix}\] <p><em>Solution.</em> (a) The product will be a $2 \times 2$ matrix.</p> \[AB = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 3 &amp; 1 &amp; 1 \end{bmatrix} \begin{bmatrix} -8 &amp; -4 \\ 9 &amp; 5 \\ -3 &amp; -2 \end{bmatrix} = \begin{bmatrix} -8+18-9 &amp; -4+10-6 \\ -24+9-3 &amp; -12+5-2 \end{bmatrix} = \boxed{\begin{bmatrix} 1 &amp; 0 \\ -18 &amp; -9 \end{bmatrix}}\] <p>The product $BA$ will be a $3 \times 3$ matrix.</p> \[BA = \begin{bmatrix} -8 &amp; -4 \\ 9 &amp; 5 \\ -3 &amp; -2 \end{bmatrix} \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 3 &amp; 1 &amp; 1 \end{bmatrix} = \begin{bmatrix} -8-12 &amp; -16-4 &amp; -24-4 \\ 9+15 &amp; 18+5 &amp; 27+5 \\ -3-6 &amp; -6-2 &amp; -9-2 \end{bmatrix} = \boxed{\begin{bmatrix} -20 &amp; -20 &amp; -28 \\ 24 &amp; 23 &amp; 32 \\ -9 &amp; -8 &amp; -11 \end{bmatrix}}\] <p>(b) The product $AB$ will be a $1 \times 2$ matrix.</p> \[AB = \begin{bmatrix} 1 &amp; 4 \end{bmatrix} \begin{bmatrix} 6 &amp; -4 \\ 3 &amp; 2 \end{bmatrix} = \begin{bmatrix} 6+12 &amp; -4+8 \end{bmatrix} = \boxed{\begin{bmatrix} 18 &amp; 4 \end{bmatrix}}\] <p>The product $BA$ is $\boxed{\text{undefined}}$ as the dimensions don‚Äôt match. $\square$</p> <hr/> <p><strong>1.3.</strong> Let</p> \[A = \begin{bmatrix} a_1 &amp; \cdots &amp; a_n \end{bmatrix}\] <p>be a row vector, and let</p> \[B = \begin{bmatrix} b_1 \\ \vdots \\ b_n \end{bmatrix}\] <p>be a column vector. Compute the products $AB$ and $BA$.</p> <p><em>Solution.</em> We have that</p> \[AB = \begin{bmatrix} a_1 &amp; \cdots &amp; a_n \end{bmatrix} \begin{bmatrix} b_1 \\ \vdots \\ b_n \end{bmatrix} = a_1b_1 + a_2b_2 + \cdots + a_nb_n = \sum_{k=1}^{n} a_k b_k\] <p>Also,</p> \[BA = \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{bmatrix} \begin{bmatrix} a_1 &amp; a_2 &amp; \cdots &amp; a_n \end{bmatrix} = \begin{bmatrix} b_1 a_1 &amp; b_1 a_2 &amp; \cdots &amp; b_1 a_n \\ b_2 a_1 &amp; b_2 a_2 &amp; \cdots &amp; b_2 a_n \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ b_n a_1 &amp; b_n a_2 &amp; \cdots &amp; b_n a_n \end{bmatrix} \square\] <hr/> <p><strong>1.7.</strong> Find a formula for</p> \[\begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}^n,\] <p>and prove it by induction.</p> <p><em>Solution.</em> Let</p> \[M = \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}.\] <p>The formula is</p> \[M^n = \begin{bmatrix} 1 &amp; n &amp; \frac{n(n+1)}{2} \\ 0 &amp; 1 &amp; n \\ 0 &amp; 0 &amp; 1 \end{bmatrix}.\] <p>For the base case $n=1$, we get</p> \[\begin{bmatrix} 1 &amp; 1 &amp; \frac{1(2)}{2} \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} = M,\] <p>which is obviously true. For the inductive step, assume the formula holds for an integer $k \ge 1$. Then</p> \[M^{k+1} = M^k M = \begin{bmatrix} 1 &amp; k &amp; \frac{k(k+1)}{2} \\ 0 &amp; 1 &amp; k \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} = \begin{bmatrix} 1 &amp; 1+k &amp; 1+k+\frac{k(k+1)}{2} \\ 0 &amp; 1 &amp; 1+k \\ 0 &amp; 0 &amp; 1 \end{bmatrix}.\] <p>The entry at position (1,3) simplifies to $\frac{2+2k+k^2+k}{2} = \frac{k^2+3k+2}{2} = \frac{(k+1)(k+2)}{2}$. Thus,</p> \[M^{k+1} = \begin{bmatrix} 1 &amp; k+1 &amp; \frac{(k+1)(k+2)}{2} \\ 0 &amp; 1 &amp; k+1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix},\] <p>as desired. $\square$</p> <hr/> <p><strong>1.13.</strong> A square matrix $A$ is <em>nilpotent</em> if $A^k=0$ for some $k&gt;0$. Prove that if $A$ is nilpotent, then $I+A$ is invertible. Do this by finding the inverse.</p> <p><em>Solution.</em> Consider the matrix $B = I - A + A^2 - \cdots + (-1)^{k-1}A^{k-1}$. It follows that the product</p> \[(I+A)B = (I+A)(I - A + A^2 - \cdots + (-1)^{k-1}A^{k-1}).\] <p>This telescopes nicely into $I - (-A)^k = I - (-1)^k A^k$. By the nilpotent condition, $A^k=0$, and so this simplifies to $I - 0 = I$; thus, $I+A$ is invertible and its inverse is $B,$ as desired. $\square$</p>]]></content><author><name></name></author><summary type="html"><![CDATA[yay artin]]></summary></entry><entry><title type="html">first post</title><link href="https://ak4774blog.github.io/blog/2025/summer-first-post/" rel="alternate" type="text/html" title="first post"/><published>2025-06-30T00:00:00+00:00</published><updated>2025-06-30T00:00:00+00:00</updated><id>https://ak4774blog.github.io/blog/2025/summer-first-post</id><content type="html" xml:base="https://ak4774blog.github.io/blog/2025/summer-first-post/"><![CDATA[<p>I can‚Äôt lie, this summer has been <em>pretty boring</em>. Usually, I feel somewhat productive during these times of the year, but with all the important milestones coming up next year, I‚Äôm feeling a bit overwhelemed, leading me to do stuff like making this blog. That is okay though, I believe that it is okay to relax during the summer (especially while being campless). Anyways, I‚Äôve been having a lot of time to do a lot of math and stuff, so be sure to tune in on future posts. I‚Äôm probably going to either do one of two things:</p> <ol> <li>post solutions to problems I thought were cool (to any subject)</li> <li>or just thought dump.</li> </ol> <p>I‚Äôm thinking about posting solutions to Artin‚Äôs Algebra, as I‚Äôm really making the push to getting at the level that I need to be for the upcoming MIT PRIMES cycle, and so I need to develop my higher math skills. I‚Äôve also been doing some OTIS as well, and if I feel like it I will post some cool problems from that as well.</p> <p>On a more fun note, I‚Äôve been playing outside with my parents a lot, and it‚Äôs really fun; usually we just play badminton and throw around a frisbee. I also signed up for summer gym to get a free for next year, which is suprisingly pretty fun, and it keeps my sleep schedule in line!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[experiences with summer before sophmore year]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://ak4774blog.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://ak4774blog.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://ak4774blog.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024 We‚Äôre introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we‚Äôre introducing Gemini 1.5 Flash: a model that‚Äôs lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We‚Äôre also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5‚Äôs 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It‚Äôs optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it‚Äôs a lighter weight model than 1.5 Pro, it‚Äôs highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it‚Äôs been trained by 1.5 Pro through a process called ‚Äúdistillation,‚Äù where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash‚Äôs availability and pricing.Over the last few months, we‚Äôve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we‚Äôve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We‚Äôve improved control over the model‚Äôs responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we‚Äôve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we‚Äôre now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do ‚Äî not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we‚Äôre also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We‚Äôre announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we‚Äôve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind‚Äôs mission to build AI responsibly to benefit humanity, we‚Äôve always wanted to develop universal AI agents that can be helpful in everyday life. That‚Äôs why today, we‚Äôre sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do ‚Äî and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we‚Äôve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we‚Äôve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we‚Äôve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they‚Äôre being used in, and respond quickly, in conversation.With technology like this, it‚Äôs easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We‚Äôve made incredible progress so far with our family of Gemini models, and we‚Äôre always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we‚Äôre able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google‚Äôs privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let‚Äôs stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We‚Äôre sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://ak4774blog.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://ak4774blog.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://ak4774blog.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio¬†Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website!¬†üéâüéâ</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as¬†sources.</p> <p>Any questions or suggestions? üëâ Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on¬†GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>